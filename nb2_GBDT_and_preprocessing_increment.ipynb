{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d3a26d",
   "metadata": {},
   "source": [
    "# Build plain Random Forest classifier on Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "48a2178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "# https://www.kaggle.com/code/neerajmohan/nlp-text-classification-using-tf-idf-features\n",
    "\n",
    "t = pd.read_excel(f'{sdir}t4m_2022_05_05__21_59_v3.xlsx')\n",
    "\n",
    "df = t[['text', 'class_name']].copy()\n",
    "df['label'] = df['class_name'].map(cls2idx)\n",
    "df['label'].value_counts()\n",
    "\n",
    "tmp = df[['text','label']].copy()\n",
    "\n",
    "data2 = tmp.copy()\n",
    "\n",
    "\n",
    "data2 = data2[['text', 'label']].set_axis(['headline', 'label'], axis=1)\n",
    "data2=Dataset.from_pandas(data2)\n",
    "\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = data2.train_test_split(test_size=0.3,seed=15)\n",
    "\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5,seed=15)\n",
    "\n",
    "# gather everything to have a single DatasetDict\n",
    "data2 = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train'],\n",
    "                   })\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "Xy_train = train_testvalid['train'].to_pandas()\n",
    "Xy_test = test_valid['test'].to_pandas()\n",
    "Xy_val = test_valid['train'].to_pandas()\n",
    "\n",
    "X_train, y_train = Xy_train['headline'], Xy_train['label']\n",
    "X_val, y_val = Xy_val['headline'], Xy_val['label']\n",
    "X_test, y_test = Xy_test['headline'], Xy_test['label']\n",
    "\n",
    "X_train = pd.concat([X_train, X_val], ignore_index=True)\n",
    "y_train = y_train.values.tolist() + y_val.values.tolist()\n",
    "\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9be3d904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier()\n",
    "\n",
    "classifier.fit(tfidf_train_vectors,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b003a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                               обновление       0.75      0.60      0.67        25\n",
      "                                   другое       0.55      0.70      0.62        47\n",
      "                                     цена       0.76      0.65      0.70        20\n",
      "                               лояльность       0.75      0.87      0.81        46\n",
      "                          создание заказа       0.38      0.50      0.43        28\n",
      "                             обслуживание       0.45      0.36      0.40        28\n",
      "                 долгое ожидание доставки       0.60      0.50      0.55        24\n",
      "                       глюки баги тормоза       0.53      0.56      0.55        48\n",
      "                                   купоны       0.65      0.76      0.70        17\n",
      "                           доставка общее       0.70      0.69      0.70        45\n",
      "                                  аккаунт       0.81      0.71      0.76        24\n",
      "                         регистрация/коды       0.86      0.79      0.83        24\n",
      "не возвращаются деньги отмененного заказа       0.71      0.83      0.76        29\n",
      "                                     uxui       0.71      0.34      0.46        44\n",
      "                                   оплата       0.57      0.71      0.63        28\n",
      "\n",
      "                                 accuracy                           0.64       477\n",
      "                                macro avg       0.65      0.64      0.64       477\n",
      "                             weighted avg       0.65      0.64      0.63       477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(tfidf_test_vectors)\n",
    "print(classification_report(y_test,y_pred, target_names=[v.replace('_', ' ') for i,v in idx2cls.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a496b33",
   "metadata": {},
   "source": [
    "# Build CatBoostClassifier on Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "066cea3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.059766\n",
      "0:\tlearn: 0.2616644\ttest: 0.2360951\tbest: 0.2360951 (0)\ttotal: 121ms\tremaining: 10m 7s\n",
      "500:\tlearn: 0.7799458\ttest: 0.6024453\tbest: 0.6041279 (498)\ttotal: 21s\tremaining: 3m 8s\n",
      "1000:\tlearn: 0.8255710\ttest: 0.6071514\tbest: 0.6108478 (949)\ttotal: 41.7s\tremaining: 2m 46s\n",
      "1500:\tlearn: 0.8532676\ttest: 0.6161412\tbest: 0.6186343 (1375)\ttotal: 1m 2s\tremaining: 2m 25s\n",
      "2000:\tlearn: 0.8677961\ttest: 0.6188620\tbest: 0.6188620 (1989)\ttotal: 1m 23s\tremaining: 2m 4s\n",
      "2500:\tlearn: 0.8800350\ttest: 0.6189548\tbest: 0.6205092 (2312)\ttotal: 1m 43s\tremaining: 1m 43s\n",
      "3000:\tlearn: 0.8954127\ttest: 0.6204597\tbest: 0.6213445 (2838)\ttotal: 2m 4s\tremaining: 1m 22s\n",
      "Stopped by overfitting detector  (550 iterations wait)\n",
      "\n",
      "bestTest = 0.6213444566\n",
      "bestIteration = 2838\n",
      "\n",
      "Shrink model to first 2839 iterations.\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                               обновление       0.69      0.88      0.77        25\n",
      "                                   другое       0.56      0.68      0.62        47\n",
      "                                     цена       0.74      0.70      0.72        20\n",
      "                               лояльность       0.76      0.85      0.80        46\n",
      "                          создание заказа       0.41      0.46      0.43        28\n",
      "                             обслуживание       0.54      0.54      0.54        28\n",
      "                 долгое ожидание доставки       0.57      0.54      0.55        24\n",
      "                       глюки баги тормоза       0.62      0.50      0.55        48\n",
      "                                   купоны       0.73      0.94      0.82        17\n",
      "                           доставка общее       0.74      0.58      0.65        45\n",
      "                                  аккаунт       0.79      0.79      0.79        24\n",
      "                         регистрация/коды       0.81      0.71      0.76        24\n",
      "не возвращаются деньги отмененного заказа       0.74      0.86      0.79        29\n",
      "                                     uxui       0.57      0.36      0.44        44\n",
      "                                   оплата       0.62      0.71      0.67        28\n",
      "\n",
      "                                 accuracy                           0.65       477\n",
      "                                macro avg       0.66      0.67      0.66       477\n",
      "                             weighted avg       0.65      0.65      0.65       477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "Xy_train = train_testvalid['train'].to_pandas()\n",
    "\n",
    "###############\n",
    "\n",
    "Xy = Xy_train.copy()\n",
    "target = 'headline'\n",
    "\n",
    "cnts = pd.DataFrame(Xy['label'].value_counts()).reset_index().set_axis(['cname', 'cnt'], axis=1).\\\n",
    "    assign(needed=lambda row: row['cnt'].max()-row['cnt'])\n",
    "\n",
    "rows2add = []\n",
    "for ind, row in cnts.iterrows():\n",
    "    smpl = Xy.query(f'label == {row[\"cname\"]}').sample(row['needed'], replace=True)\n",
    "    for ind2, row2 in smpl.iterrows():\n",
    "        rows2add.append([random_insert2str(row2[target]), row2['label']])\n",
    "\n",
    "Xy = pd.concat([Xy, pd.DataFrame(rows2add, columns = Xy.columns)]).sample(frac=1)\n",
    "\n",
    "Xy_train = Xy.copy()\n",
    "\n",
    "###############\n",
    "\n",
    "\n",
    "Xy_test = test_valid['test'].to_pandas()\n",
    "Xy_val = test_valid['train'].to_pandas()\n",
    "\n",
    "X_train, y_train = Xy_train['headline'], Xy_train['label']\n",
    "X_val, y_val = Xy_val['headline'], Xy_val['label']\n",
    "X_test, y_test = Xy_test['headline'], Xy_test['label']\n",
    "\n",
    "tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test_vectors = tfidf_vectorizer.transform(X_test)\n",
    "tfidf_val_vectors = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))\n",
    "\n",
    "params_cat = {\n",
    "      'eval_metric':'TotalF1', \n",
    "      'iterations':5000,\n",
    "      'verbose':500,\n",
    "      'early_stopping_rounds':550,\n",
    "      'border_count': 254,\n",
    "      'use_best_model':True,\n",
    "      'class_weights':class_weights,\n",
    "    'task_type':'CPU'\n",
    "  }\n",
    "\n",
    "\n",
    "model = CatBoostClassifier(**params_cat)\n",
    "preds = model.fit(tfidf_train_vectors, y_train,\n",
    "                 eval_set=(tfidf_val_vectors, y_val)\n",
    "                 ).predict(tfidf_test_vectors)\n",
    "\n",
    "print(classification_report(y_test, preds, target_names=[v.replace('_', ' ') for i,v in idx2cls.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c873a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38906f18",
   "metadata": {},
   "source": [
    "# Prepare for random insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d1c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_ins = [\n",
    "    'app store',\n",
    "    'google play',\n",
    "    'burger king',\n",
    "#     'macdonalds',\n",
    "#     'rostics',\n",
    "#     'kfc',\n",
    "    'ресторан стоит',\n",
    "    'телефон работает',\n",
    "    'еду кушают',\n",
    "    'ночью спят',\n",
    "    'днем работают',\n",
    "    'обед днем',\n",
    "    'завтрак утром',\n",
    "    'ужин вечером'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef60faa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insert2str(s0):\n",
    "    \n",
    "    s = s0.strip()\n",
    "    spaces = [i for i, ltr in enumerate(s) if ltr == ' ']\n",
    "    inserts = []\n",
    "    if len(spaces) > 0:\n",
    "        inserts=np.random.choice(rand_ins, len(spaces)//5 + 1)\n",
    "    else:\n",
    "        return s + ' ' + np.random.choice(rand_ins, 1)[0]\n",
    "\n",
    "    spaces2ins = np.random.choice(spaces, len(inserts))\n",
    "\n",
    "    ss = ''\n",
    "    i0 = 0\n",
    "    for ind, i in enumerate(spaces2ins):\n",
    "        ss += s[i0:i] + f' {inserts[ind]}'\n",
    "        i0 = i\n",
    "    if i0 < len(s):\n",
    "        ss += s[i0:]\n",
    "    return ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac40c23",
   "metadata": {},
   "source": [
    "# Load file and define function to compare F1 with different data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6a17c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from datasets import load_dataset,Dataset,DatasetDict, load_metric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# https://www.kaggle.com/code/neerajmohan/nlp-text-classification-using-tf-idf-features\n",
    "\n",
    "t = pd.read_excel(f'{sdir}t4m_2022_05_05__21_59_v3.xlsx')\n",
    "cnames = list(set(t['class_name'].values))\n",
    "idx2cls = {i:v for i,v in enumerate(cnames)}\n",
    "cls2idx = {v:i for i,v in enumerate(cnames)}\n",
    "\n",
    "df = t[['content', 'text_1', 'text_2', 'text_3_0', 'text_3', 'text_5', 'text_6', 'class_name']].copy()\n",
    "df['label'] = df['class_name'].map(cls2idx)\n",
    "df['label'].value_counts()\n",
    "\n",
    "tmp = df[['content', 'text_1', 'text_2', 'text_3_0', 'text_3', 'text_5', 'text_6','label']].copy()\n",
    "\n",
    "\n",
    "data2 = tmp.copy()\n",
    "\n",
    "\n",
    "data2 = data2[['content', 'text_1', 'text_2', 'text_3_0', 'text_3', 'text_5', 'text_6', 'label']].copy()\n",
    "data2=Dataset.from_pandas(data2)\n",
    "\n",
    "# 80% train, 20% test + validation\n",
    "train_testvalid = data2.train_test_split(test_size=0.3,seed=15)\n",
    "\n",
    "# Split the 10% test + valid in half test, half valid\n",
    "test_valid = train_testvalid['test'].train_test_split(test_size=0.5,seed=15)\n",
    "\n",
    "# gather everything to have a single DatasetDict\n",
    "data2 = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train'],\n",
    "                   })\n",
    "\n",
    "# tfidf_vectorizer = TfidfVectorizer() \n",
    "\n",
    "Xy_train = train_testvalid['train'].to_pandas()\n",
    "Xy_test = test_valid['test'].to_pandas()\n",
    "Xy_val = test_valid['train'].to_pandas()\n",
    "\n",
    "\n",
    "def score_text(target, out=False, random_insert=False):\n",
    "    X_train, y_train = Xy_train[target], Xy_train['label']\n",
    "    X_val, y_val = Xy_val[target], Xy_val['label']\n",
    "    X_test, y_test = Xy_test[target], Xy_test['label']\n",
    "\n",
    "    X_train = pd.concat([X_train, X_val], ignore_index=True)\n",
    "    y_train = y_train.values.tolist() + y_val.values.tolist()\n",
    "    \n",
    "\n",
    "    ##################################\n",
    "\n",
    "    if random_insert:\n",
    "        Xy = pd.DataFrame({target:X_train.values.tolist(), 'label':y_train})\n",
    "\n",
    "        cnts = pd.DataFrame(Xy['label'].value_counts()).reset_index().set_axis(['cname', 'cnt'], axis=1).\\\n",
    "            assign(needed=lambda row: row['cnt'].max()-row['cnt'])\n",
    "\n",
    "        rows2add = []\n",
    "        for ind, row in cnts.iterrows():\n",
    "            smpl = Xy.query(f'label == {row[\"cname\"]}').sample(row['needed'], replace=True)\n",
    "            for ind2, row2 in smpl.iterrows():\n",
    "                rows2add.append([random_insert2str(row2[target]), row2['label']])\n",
    "\n",
    "        Xy = pd.concat([Xy, pd.DataFrame(rows2add, columns = Xy.columns)]).sample(frac=1)\n",
    "\n",
    "        X_train = Xy[target]\n",
    "        y_train = Xy['label'].values.tolist()\n",
    "\n",
    "    ##################################\n",
    "\n",
    "    tfidf_vectorizer = TfidfVectorizer() \n",
    "    \n",
    "    tfidf_train_vectors = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "    tfidf_test_vectors = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    classifier.fit(tfidf_train_vectors,y_train)\n",
    "\n",
    "    y_pred = classifier.predict(tfidf_test_vectors)\n",
    "    if out: print(classification_report(y_test,y_pred))\n",
    "    \n",
    "    return classification_report(y_test,y_pred, output_dict=True, target_names=list(cls2idx.keys()))\n",
    "\n",
    "def score_text2(target, n=15, random_insert=False):\n",
    "    res = []\n",
    "    for i in range(n):\n",
    "        res.append(score_text(target, random_insert=random_insert)['macro avg']['f1-score'])\n",
    "        \n",
    "    return np.mean(np.array(res)), np.std(np.array(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d836f83",
   "metadata": {},
   "source": [
    "# Find out F1 increment for each data preparation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f2bc3eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing content: F1 = 0.628 ± 0.011\n",
      "testing text_1: F1 = 0.630 ± 0.011\n",
      "testing text_2: F1 = 0.627 ± 0.009\n",
      "testing text_3_0: F1 = 0.644 ± 0.010\n",
      "testing text_3: F1 = 0.641 ± 0.009\n",
      "testing text_5: F1 = 0.645 ± 0.009\n"
     ]
    }
   ],
   "source": [
    "for param in ['content', 'text_1', 'text_2', 'text_3_0', 'text_3', 'text_5', 'text_6']:\n",
    "    f,s = score_text2(param, n=25)\n",
    "    print(f'testing {param}: F1 = {f:.3f} ± {s:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f9362",
   "metadata": {},
   "source": [
    "# Random insert increment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f638c4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing text_5: F1 = 0.651 ± 0.009\n"
     ]
    }
   ],
   "source": [
    "param = 'text_5'\n",
    "f,s = score_text2(param, n=5, random_insert=True)\n",
    "print(f'testing {param}: F1 = {f:.3f} ± {s:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93ae86",
   "metadata": {},
   "source": [
    "# Random insert increment wo business replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ecc99c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing text_6: F1 = 0.651 ± 0.003\n"
     ]
    }
   ],
   "source": [
    "param = 'text_6'\n",
    "f,s = score_text2(param, n=5, random_insert=True)\n",
    "print(f'testing {param}: F1 = {f:.3f} ± {s:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe26557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
